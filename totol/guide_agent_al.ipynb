{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c6e8c4",
   "metadata": {},
   "source": [
    "# 主动学习智能策略（Agent + LLM）使用指南\n",
    "\n",
    "本笔记介绍 totol 目录中实现的两种方案：\n",
    "- 方案1：多臂老虎机（Exp3风格）权重更新 + 融合排序（StrategyAgent）。\n",
    "- 方案2：LLM/规则决策器（LLMStrategyDecider），默认规则回退。\n",
    "\n",
    "并演示实际的文件读取与保存位置：\n",
    "- 数据读取：/home/v-wenliao/gnot/GNOT/data/al_bz 下的 al_labeled.pkl、al_unlabeled.pkl、al_test.pkl\n",
    "- 临时评估：/home/v-wenliao/gnot/GNOT/data/al_bz/data 下的临时 *.pkl（由 StrategyAgent 在奖励评估时使用）\n",
    "- 策略权重持久化：/home/v-wenliao/gnot/GNOT/totol/state.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pickle, time, numpy as np\n",
    "ROOT = '/home/v-wenliao/gnot/GNOT'\n",
    "if ROOT not in sys.path: sys.path.append(ROOT)\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT, 'data', 'al_bz')\n",
    "STATE_PATH = os.path.join(ROOT, 'totol', 'state.json')\n",
    "TEMP_DIR = os.path.join(DATA_DIR, 'data')  # StrategyAgent 内部也会用到\n",
    "\n",
    "print('ROOT =', ROOT)\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('STATE_PATH =', STATE_PATH)\n",
    "print('TEMP_DIR =', TEMP_DIR)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e6d30",
   "metadata": {},
   "source": [
    "## 目录结构与关键文件\n",
    "- totol/agent_selector.py：方案1，权重更新+融合排序的智能策略选择器。\n",
    "- totol/strategy_registry.py：注册/封装现有策略，提供回退策略。\n",
    "- totol/llm_selector.py：方案2，LLM/规则决策器。\n",
    "- totol/local_strategies.py：随机/幅值/多样性回退策略。\n",
    "- totol/mock_env.py：合成数据与轻量模型，便于快速演示。\n",
    "- totol/run_agent_al.py：命令行演示脚本（bandit/llm/hybrid）。\n",
    "- totol/state.json：跨轮的策略权重持久化文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from totol.mock_env import create_synthetic_al_split\n",
    "# 若无数据，生成少量合成数据到 DATA_DIR\n",
    "lp = os.path.join(DATA_DIR, 'al_labeled.pkl')\n",
    "up = os.path.join(DATA_DIR, 'al_unlabeled.pkl')\n",
    "tp = os.path.join(DATA_DIR, 'al_test.pkl')\n",
    "if not (os.path.exists(lp) and os.path.exists(up) and os.path.exists(tp)):\n",
    "    create_synthetic_al_split(DATA_DIR, n_labeled=8, n_unlabeled=60, n_test=12)\n",
    "\n",
    "def pkl_info(p):\n",
    "    sz = os.path.getsize(p) if os.path.exists(p) else 0\n",
    "    return {'path': p, 'exists': os.path.exists(p), 'size_bytes': sz}\n",
    "\n",
    "print('Data files:')\n",
    "print(pkl_info(lp))\n",
    "print(pkl_info(up))\n",
    "print(pkl_info(tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48077f9",
   "metadata": {},
   "source": [
    "## 方案1：Agent（多臂老虎机 + 融合排序）\n",
    "- 每轮仅训练一次模型（如 albz.train_model，示例中用 mock 快速演示）。\n",
    "- 对每个策略在小候选集上估计“奖励”（误差越大→潜在收益越高）。\n",
    "- 用 Exp3 累乘更新策略权重。\n",
    "- 将各策略 Top-K 名单按权重做名次分融合，得全局排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from totol.strategy_registry import build_default_strategies\n",
    "from totol.agent_selector import StrategyAgent, ALBZ, TEMP_DIR as AGENT_TEMP\n",
    "from totol.mock_env import build_mock_model_tuple\n",
    "\n",
    "# 加载数据\n",
    "import pickle\n",
    "with open(lp, 'rb') as f: labeled = pickle.load(f)\n",
    "with open(up, 'rb') as f: unlabeled = pickle.load(f)\n",
    "with open(tp, 'rb') as f: test = pickle.load(f)\n",
    "print(f'labeled={len(labeled)}, unlabeled={len(unlabeled)}, test={len(test)}')\n",
    "\n",
    "# 为了快速演示，使用轻量 mock 模型（若需真实训练可切换为 albz.train_model）\n",
    "model_tuple = build_mock_model_tuple(labeled[0])\n",
    "\n",
    "# 构建策略集合（可只保留本地回退策略以保证轻量运行）\n",
    "all_specs = build_default_strategies()\n",
    "FAST_DEMO = True\n",
    "if FAST_DEMO:\n",
    "    specs = {k: v for k, v in all_specs.items() if k in ('random','magnitude','diversity')}\n",
    "else:\n",
    "    specs = all_specs\n",
    "\n",
    "print('Discovered strategies:', list(specs.keys()))\n",
    "\n",
    "# 初始化智能选择器\n",
    "agent = StrategyAgent(strategies=specs, candidate_size=8)\n",
    "print('Weights before:', json.dumps(agent.weights, indent=2))\n",
    "\n",
    "# 选择样本索引\n",
    "selected = agent.select_indices(model_tuple, labeled, unlabeled, select_num=20)\n",
    "print('Selected indices (Agent):', selected[:20])\n",
    "print('Weights after:', json.dumps(agent.weights, indent=2))\n",
    "\n",
    "print('State saved to:', STATE_PATH, 'exists?', os.path.exists(STATE_PATH))\n",
    "print('Agent TEMP_DIR:', AGENT_TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3be38",
   "metadata": {},
   "source": [
    "## 查看持久化的权重文件 state.json（保存位置演示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54845d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(STATE_PATH):\n",
    "    with open(STATE_PATH, 'r') as f: print(f.read())\n",
    "else:\n",
    "    print('state.json not found at', STATE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7eb13",
   "metadata": {},
   "source": [
    "## 方案2：LLM/规则决策器（可插拔）\n",
    "- 将最近轮的指标（例如 metrics.csv 聚合）输入决策器。\n",
    "- 未配置 LLM API 时，使用规则回退选择策略。\n",
    "- 可与方案1结合（Hybrid），作为首选策略来源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from totol.llm_selector import LLMStrategyDecider\n",
    "from totol.strategy_registry import StrategySpec\n",
    "\n",
    "decider = LLMStrategyDecider()\n",
    "recent_metrics = {\n",
    "    'pressure':   [0.12, 0.10],\n",
    "    'wall-shear': [0.02, 0.02]\n",
    "}\n",
    "cand = list(specs.keys())\n",
    "choice = decider.decide(recent_metrics, cand)\n",
    "print('LLM/规则选中的策略:', choice)\n",
    "\n",
    "# 直接调用该策略（统一签名）\n",
    "spec = specs[choice]\n",
    "sel_llm = spec.func(model_tuple, labeled, unlabeled, select_num=20)\n",
    "print('Selected indices (LLM choice):', sel_llm[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d78163",
   "metadata": {},
   "source": [
    "## Hybrid示例：LLM首选 + Agent融合\n",
    "- 先用 LLM 决策一个首选策略，\n",
    "- 再与 Agent 融合结果，形成最终选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1871e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "half = 10\n",
    "base_sel = sel_llm[:max(half, 16)] if sel_llm else []\n",
    "agent_sel = agent.select_indices(model_tuple, labeled, unlabeled, select_num=20)\n",
    "# 合并去重\n",
    "hybrid_sel = list(dict.fromkeys(base_sel[:half] + agent_sel))[:20]\n",
    "print('Selected indices (Hybrid):', hybrid_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fbe25",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- 数据读取：\n",
    "  - `al_labeled.pkl`, `al_unlabeled.pkl`, `al_test.pkl` 来自 `data/al_bz/`。\n",
    "- 临时保存：\n",
    "  - 奖励评估时，`totol/agent_selector.py` 会在 `data/al_bz/data/` 下写入/清理临时 `*.pkl`。\n",
    "- 权重保存：\n",
    "  - `totol/state.json` 存储跨轮的策略权重。\n",
    "- 两种方案可独立使用，也可 **Hybrid** 组合。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
